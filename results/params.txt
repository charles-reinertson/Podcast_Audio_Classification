Base model:
	Optimizer:  Adam
	Loss:  Cross Entropy
	Epochs:  41
	Learning Rate:  0.0001
	Batch Size:  64
	Network:
		FC1:  29404 --> 10000
		Dropout:  0.4
		Activation: SiLU
		FC2:  10000 --> 1000
		Dropout:  0.4
		Activation: SiLU
		FC3:  1000 --> 11

Audio features:
	Network:
		FC1: 88 --> 44
		FC2: 44 --> 22
		FC3: 22 --> 11

Emotion features:
	Network:
		FC1: 15 --> 13
		FC2: 13 --> 11
		FC3: None

Bag of Words:
	Epochs: 10

BERT pooled:
	Network:
		FC1: 768 --> 256
		FC2: 256 --> 64
		FC3: 64 --> 11

BERT:
	Epochs: 16
	Network:
		FC1: 768 --> 256
		FC2: 256 --> 64
		FC3: 64 --> 11

DistilBERT:
	Epochs: 16
	Network:
		FC1: 768 --> 256
		FC2: 256 --> 64
		FC3: 64 --> 11

Bag of words and BERT:
	Epochs: 16
	Network:
		FC1 30172 --> 10000
