Base model:
	Optimizer:  Adam
	Loss:  Cross Entropy
	Epochs:  41
	Learning Rate:  0.0001
	Batch Size:  64
	Network:
		FC1:  29404 --> 10000
		Dropout:  0.4
		Activation: SiLU
		FC2:  10000 --> 1000
		Dropout:  0.4
		Activation: SiLU
		FC3:  1000 --> 11

Bag of words encoded transcripts (bag_of_words):
	Epochs: 9
	Validation Accuracy:   46%

DistilBERT encoded Transcripts (distilbert):
	Epochs:  16
	Network:
		FC1: 768 --> 256
		FC2: 256 --> 64
		FC3: 64 --> 11
	Validation Accuracy:  51.0 %

BERT encoded Transcripts (bert):
	Epochs:  16
	Network:
		FC1: 768 --> 256
		FC2: 256 --> 64
		FC3: 64 --> 11
	Validation Accuracy:  50.1 %

BERT encoded Transcripts with pooling layer (bert_pooled):
	Network:
		FC1: 768 --> 256
		FC2: 256 --> 64
		FC3: 64 --> 11
	Validation Accuracy:  43.7 %
	
Emotion NRCLex features (emotion):
	Epochs: 40
	Network:
		FC1: 15 --> 13
		FC2: 13 --> 11
	Validation Accuracy:  14.2 %
	
Audio features (audio):
	Epochs: 40
	Network:
		FC1: 88 --> 44
		FC2: 44 --> 22
		FC3: 22 --> 11
	Validation Accuracy:  11.2 %
